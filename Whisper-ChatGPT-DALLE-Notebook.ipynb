{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Ecosystem\n",
        "Bhavesh Bhatt\n",
        "\n",
        "[**Link to my YouTube Channel**](https://www.youtube.com/BhaveshBhatt8791?sub_confirmation=1)"
      ],
      "metadata": {
        "id": "vjUw8n7qFCyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "ueQCRH0EFzya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "RUAaMwrpH81X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JnmEH7vtF1qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import json"
      ],
      "metadata": {
        "id": "MPrjwstWIgGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Key"
      ],
      "metadata": {
        "id": "S2N29i1XF3Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('GPT_SECRET_KEY.json') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "btYuk2MzKiYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = data[\"API_KEY\"]"
      ],
      "metadata": {
        "id": "sFXG2nL2chHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT API Function"
      ],
      "metadata": {
        "id": "x_cXBPLcF4_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_api(input_text):\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "    \n",
        "    if input_text:\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": 'Summarize this text \"{}\" into a short and concise Dall-e2 prompt'.format(input_text)},\n",
        "        )\n",
        "        \n",
        "        chat_completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "    \n",
        "    reply = chat_completion.choices[0].message.content\n",
        "    return reply"
      ],
      "metadata": {
        "id": "bW6rWVSeKibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DALL-E API Function"
      ],
      "metadata": {
        "id": "quSx2YkPF7fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dall_e_api(dalle_prompt):\n",
        "\n",
        "    dalle_response = openai.Image.create(\n",
        "            prompt = dalle_prompt,\n",
        "            size=\"512x512\"\n",
        "        )\n",
        "    image_url = dalle_response['data'][0]['url']\n",
        "    return image_url"
      ],
      "metadata": {
        "id": "xWrueQ3TKluo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper API Function"
      ],
      "metadata": {
        "id": "ALZaayYcF91g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def whisper_transcribe(audio):\n",
        "    \n",
        "    audio_file = open(audio, \"rb\")\n",
        "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "    dalle_prompt = chatgpt_api(transcript[\"text\"])\n",
        "    image_url = dall_e_api(dalle_prompt)\n",
        "    return transcript[\"text\"], image_url"
      ],
      "metadata": {
        "id": "e7dCzTUxMZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "tyvKELESGAPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_1 = gr.Textbox(label=\"Speech to Text\")\n",
        "output_2 = gr.Image(label=\"DALL-E Image\")"
      ],
      "metadata": {
        "id": "kJm11xAicLg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "pGK7X2HiH4D_",
        "outputId": "03ba7a0b-e7d5-471b-900d-a8cb3fc986b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "speech_interface = gr.Interface(fn = whisper_transcribe, \n",
        "                                inputs = gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
        "                                outputs = [output_1, output_2], \n",
        "                                title = \"Generate Images using Voice\")\n",
        "\n",
        "speech_interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DcOMIxrDIkDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
